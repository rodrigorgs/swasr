% TODO: CITACAO DEPOIS DO CONCEITO
In the context of our research, we consider classes as the components

\section{Abstract}

Software modularization recovery algorithms automatically recognize a system's
modular structure by analyzing its implementation.
Due to the lack of well document software systems, though, the issue of testing
these algorithms is still underexplored, limiting both their adoption in the
industry and the development of better algorithms.
We propose to rely on software models to produce arbitrarily large test sets. In
this paper we consider three such models and analyze how similar the artifacts
they produce are from artifacts from real software systems.
\cite{Pollet2007}

\section{Introduction}

Development of large-scale software systems is a challenge.

A key to success is the ability to decompose a system into weakly-coupled
modules, so each module can be developed by a distinct team. Failing to do so
results in duplicated code, non-parallelism, one's work impacting another's work
etc.

The ability do modularize depends decisively on a vast knowledge about the
system, how its different parts interact to accomplish the system's goal.

Unfortunately, in the case of legacy systems, such knowledge isn't available.
Depending on its size, it might take months to understand the system so well as
to find a good modularization. XXX \cite{Parnas1972}

POR ISSO SURGIRAM software modularization recovery algorithms, also known as
software clustering algorithms or software architecture recovery algorithms. In
its most common flavor, these algorithms analyze the dependencies between
implementation components, such as classes, and then group them into modules
such as there are few dependencies between classes in distinct modules.

Software modularization recovery algorithms can, therefore, do in minutes what a
person would spend weeks or months. The question is: are the found
modularizations good? Are they similar to what a person would find? To answer
this question it's essencial to perform empirical evaluations envolving systems
with known reference modularizations.

The empirical evaluations consist of selecting a collection of systems with
known reference modularizations and then applying the algorithms to the systems.
The modularizations found by the algorithms are then compared to the reference
decompositions by a metric such as MoJo \cite{Tzerpos1999} or PrecisionRecall.

  \begin{figure*}[!t]
  \centering
  \includegraphics[width=1.0\textwidth]{diagram}
  \caption{Evaluation of a software modularization recovery algorithm}
  \label{fig:diagram}
  \end{figure*}

% TODO: diagram showing the evaluation of a clustering algorithm

Unfortunately there are few systems with known reference modularizations and,
because to obtain reference modularizations is costly, there are few empirical
studies, and most of them consider a couple of small and medium systems.

We therefore propose to use synthetic, i.e., computer-generated, software
dependency networks, to evaluate software modularization recovery algorithms.
These networks are generated by parametrizable models and have an embedded
reference modularizations. The goal of an algorithm is, thus, to find
modularizations that are similar to the reference modularization embedded in the
network. With this approach we can CONTAR COM a large volume of test data that
is composed of networks of different sizes and controllable characteristics.

Of course the success of this approach depends on the realism of the synthetic
networks, ie, how well they resemble networks extracted from real software
systems. In this paper we study three models and show that all of them are, by
means of a careful parameter choosing, capable of producing realistic software
networks.

The remaining sections are organized as follows. .... Section 2, ...

%alike, resembling, exchangeable, indistiguible

\section{Related Work}

\section{Software Modularization Recovery: an Overview}

Aka software architecture recovery, software architecture reconstruction,
software clustering...

It's an task of reverse engineering, as defined by Tonella \cite{Tonella2007}...

DEFINITIONS

external edge...

%\section{Software Networks and Software Clustering}

directed graph, (un)weighted

\section{Network Theory}

Network theory research studies general properties of many types of networks by
using statistical analysis. In the last decade, it has been found that many
networks arising from sociology, biology, technology and other domains present
remarkable structural similarities. It has been shown that, in these networks,
the number of vertices connected to k edges, N(k), is proportional to
$k^{\-gamma}$, where $\gamma$ is a positive constant. These networks were called
scale-free networks.

Network theory has been applied to software networks and it was shown that they
are also scale-free networks \cite{Myers2003,Valverde2003}.

(in fact we can't argue the degree distribution is perfectly fit by a power law.
Anyway, the distribution is much more assymetric than normal ou Poisson's)

software indegree distribution (power law), outdegree distribution (not so power
law)

\section{Network Models}

Many models were proposed to explain the formation of scale-free networks. These
models are simple algorithms that can be proven, either formally or empirically,
to generate networks that are scale-free. In this section we present three
models that generate directed networks with built-in modular decomposition:
BCR+, CGW and LR. The first two models grow networks using a mechanism called
preferential attachment CITE Barabasi, by which nodes with many edges tend to
receive more edges in the process.

%\subsection{ER (Erdos-Renyi)}

%\subsection{Configuration model}
% take the most typical software system and steal its degree sequence

\subsection{BCR plus}

The BCR model aims to model the network of hyperlinks between web pages as a
directed graph without modules \cite{Bollobas2003}. It was proven analitically
to generate scale-free networks. We have developed an extension to this model,
which we will call BCR+, that adds the concept of module. The model accepts the
following parameters:

\begin{itemize}
\item number of vertices, $n$;
\item a graph, $G$;
\item three probabilities, $p$, $q$, and $r$, such as $p + q + r = 1$;
\item a probability, $mu$;
\item two real numbers, din and dout.
\end{itemize}

In a network with modules, one can define a MDG as a graph where the two
following conditions hold:
* each vertex represents a module in the original network;
* there is an edge from M1 to M2 in the MDG only if there is an edge from 
v1 to v2 in the original network, where v1 in M1 and v2 in M2.

The BCR+ model generates networks whose MDG is equal to G by adding vertices and
edges to an initial network until it reaches n vertices. The initial network is
isomorphic to G: it contains one vertex for each module and one edge for each
edge in G, connecting vertices whose corresponding modules are are connected.
Thus, the networks's MDG is equal to G from the very beginning.

After that, the algorithm consists of successive applications of one out of
three operations on the network: (1) adding a vertex with an outgoing edge; (2)
adding a vertex with an ingoing edge; (3) adding a edge between existing
vertices. The choice of the vertices that will be connected by a new edge,
although non-deterministic, is not fully random. The probability that a
particular vertex v is choosen, P(v), is proportional to a function of the
in-degree or of the out-degree of the node. We say that we choose a vertex
within a set V according to a function f if

  P(choose v) = f(v) / sum(v in V, f(v))

The denominator is a normalizing factor that assures that the probabilities sum
to 1.

Before we present the algorithm in detail, using pseudo-code, we must explain
some definitions. V is the set of vertices in the network being generated. This
set grows as the algorithm is executed. Being v a vertex, we define the
following three functions:

* out-neighbors(v): the set of all vertices that are connected to v by an edge
starting in v.
* same-module(v): the set of all vertices that are in the same module as v (except
v itself).
* other\_modules(v): the set of all vertices that are in modules that are connected
to v's module (in the graph G) by an edge starting in v's module.

After the initial network is created, the algorithm proceeds as follows:

while the network has less than n vertices:
  choose one of the operations according to probabilities (p, q, r)
  operation 1: (add a vertex with an outgoing edge)
    choose a vertex w within V according to f(x) = din + in-degree(x)
    add a new vertex v to w's module
    add an edge from v to w
  operation 2: (add a vertex with an ingoing edge)
    choose a vertex w within V according to f(x) = dout + out-degree(x)
    add a new vertex v to w's module
    add an edge from w to v
  operation 3: (add an edge between two existing vertices)
    choose a vertex v within V according to f(x) = dout + out-degree(x)
    choose a case according to probabilities (mu, 1 - mu)
    case 1: (distinct modules)
  	choose a vertex w within (other\_modules(v) - neighbors(v)) according to f(x) = din + in-degree(x)
      add an edge from v to w
    case 2: (same module)
      choose w within (same-module(v) - neighbors(v)) according to f(x) = din + in-degree(x)
      add an edge from v to w
  end

We can see that the probabilities p, q and r control how often each operation is
performed. Because the operation associated with probability r does not add any
vertices, greater values of r imply more edges in the resulting network. It is
easy to see that nodes connected to many ingoing edges are more likely to get
another ingoing edge, and the same reasoning can be applied to outgoing edges.
The parameter din can alleviate the handicap by providing a ``base in-degree''
that is applied to all vertices when computing the probabilities. Consider
two vertices, v1 with in-degree 4, and v2 with in-degree 8. If din = 0, v2 is
twice more likely to receive a new incoming edge; if, otherwise, din = 4, v2 is
only 3/2 more likely to receive the edge.

The parameter mu controls the proportion of edges between vertices in distinct
modules. Lower values of mu lend to networks that are more modular.

The BCR+ model is a growth model, meaning that the network is generated vertex
by vertex, growing from an initial network. It can, therefore, simulate the
evolution of a software network. Moreover, it can simulate the evolution of a
software system subject to constraints in module interaction, as is the case
with top-down design methodologies CITE.

%* Add a node with ongoing edge. With probability p, a new vertex is added to the network, together with an
%edge from the new vertex to an existing vertex, chosen preferentially according
%to .... Considering din = 0, this means that a vertex with indegree = 8 is
%twice more likely to receive the edge than a vertex with indegree = 4. The
%parameter din can be used to alleviate the handicap. (If din = 4, the first
%vertex will only be 3/2 more likely to receive the edge). The new vertex is
%put on the same cluster as the old vertex.
%
%* Add a node with ingoing edge. With probability q, a new vertex is added to the network, together with an
%edge from an existing vertex, chosen preferentially accordingly to dout +
%outdegree, to the new vertex. The new vertex is put on the same cluster as the
%old vertex. This case is similar to the previous case.
%
%* Add an edge. With probability r, a new edge is added between two vertices, v and w. v is
%chosen according to .... After that, with prob X, w is chosen from one of the
%modules connected to v's module, else it's chosen from v's own module. In any
%case, the exact choice is made according to ...
%
%It's a growth model, that is, it can take as input an existing network and
%evolve it.
%
%TODO: Argument: why the imposed modules are natural modules?

\subsection{CGW}

The CGW model was proposed to model the evolution of software systems organized in
modules. It was proven formally to generate scale-free networks. \cite{Chen2008}
It accepts the following parameters:

* n, m
* Probabilities p1, p2, p3, p4, summing 1
* Natural numbers e1, e2, e3, e4
* alpha

Just like BCR+, this is a growth model. Its initial network is composed of two
vertices belonging to the same module and a directed edge between them. The
remaining m - 1 modules are initially empty. Because the original implementation
of the model is not available, we describe in detail the algorithm we
implemented:

while the network has less than n vertices:
  choose operation (p1, p2, p3, p4)
  operation 1: (adding a vertex with e1 edges)
    create a new vertex v and add it to a randomly chosen module
    do e1 times:
      choose a vertex w according to mbpa(v)
      add an edge from v to w
  operation 2: (adding e2 edges)
    do e2 times:
      choose a vertex v randomly
      choose a vertex w according to mbpa(v)
      add edge from v to w
  operation 3: (rewiring e3 edges)
    do e3 times:
      choose a vertex v randomly
      choose an edge e randomly within edges that start in v
      choose w according to mbpa(v)
      remove e
      add an edge from v to w
  operation 3: (removing e4 edges)
    do e4 times:
      remove and randomly chosen edge
    

Unlike BCR+, this model does not allow constraints on the connection between
modules, however it accounts for the rewiring and the removal of edges. 

\subsection{LF}

The LF model is a very flexible model that can generated weighted directed
networks with overlapping modules, that is, in which a vertex can belong to more
than one module. Unlike the previous models, this is not a growth model: all
vertices are generated at once and then the edges are added.

There is also a special version of the model in which the edges weights are
discarded and the modules are non overlapping. We used the original
implementation of this version, available at http://. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Characterization of Software Networks}

Our research hypothesis is that at least one of the presented models can
synthesize networks that resemble software networks. A central issue, thus, is
how to measure similarity between networks. In order to be useful, the metric
must be able to differentiate between software and non-software networks. In
this section we present such a metric, together with an experiment that
evaluates its usefulness by applying it to both software and non-software
networks. %Then we show a classification model 

\subsection{A Metric for the Similarity Between Networks}

In a recent work, Milo et al. \cite{Milo2002} proposed to characterize networks
by analyzing their triad concentration. A triad is a network with three vertices
in which all vertices are connected. There are only 13 distinct triads, one for
each configuration of directed edges, as shown in Figure \ref{fig:triads}.

\begin{figure*}[!t]
\centering
\includegraphics[width=1.0\textwidth]{triads}
\caption{Triads (id vs. graph. reprs.)}
\label{fig:triads}
\end{figure*}

By counting how many times each triad appears in a network, one can build a
triad concentration profile (TCP), which is a vector with 13 numbers that
summarize the local structure of the network. Figure \ref{fig:profiles} shows
the TCP for three distinct networks.

Similarity between two networks can be measured by computing Pearson's
correlation coefficient between the corresponding TCPs, which yields a value
between -1 (most dissimilar) and 1 (most similar). This is the similarity metric
used in this research.

\begin{figure*}[!t]
\center
\subfigure[ref1][Software
network]{\includegraphics[width=2.5in]{triads-software}}
\hfil
\subfigure[ref2][Non-software network]{\includegraphics[width=2.5in]{triads-nonsw}%
}
\caption{Triad profiles}
\label{fig:profiles}
\end{figure*}

\subsection{Data Set}

To support the evaluation of the metric, we have collected 131 networks. The
networks are described in detail in Appendix A.

\textbf{Software networks}. We have collected 65 software systems written in
Java, with size ranging from 111 to 35,363 classes. Java was chosen for being a
popular programming language in which many open source systems were written. The
software networks, representing dependencies between classes, were extracted
with the tool Dependency Finder CITE.

\textbf{Non-software networks}. We have collected 66 networks from distinct
domains, such as biology, sociology, technology, and linguistics. These networks
are freely available on the Internet and have been used in previous researches.

% S-score

\subsection{Evaluation of the Similarity Metric}

In order to evaluate the similarity metric, we measured the similarity between
the networks in the data set. A suitable metric must fullfil two conditions: (i)
it must yield high similarity between software networks, and (ii) it must yield
lower similarity between software networks and networks from other domains.

We measured the similarity between each pair of software networks and then
calculated the average and the standard deviation. The average similarity was
0.97 and the standard deviation, 0.03. The high average similarity and the low
standard deviation show that software networks have common structural patterns
that are captured by the metric. The minimum similarity was 0.83.

TODO: Boxplot by network class (software, metabolic, neural, social etc.)

Then, for each non-software network, we measured the average similarity between
the network and all software networks. The majority of the networks (X\%) had an
average similarity lower than 0.83, which shows that they are ... Networks
extracted from the social network Facebook had negative similarities.

A few networks, though, showed high similarity to software networks. The network
polblogs, showing links between blogs on politics, had similarity 0.97. The
neural network of C Elegans also had a high similarity (0.88). Further
investigation is needed in order to discover why does it happen and whether
auxiliary metrics can differentiate these networks from software networks.

TODO: histogram of similarities? (maybe boxplot is enough)

\subsection{A Network Classification Model}

Although the similarity metric tells how close a network is from software
networks, it does not tell whether a network is close enough that it can be
considered a software network. Therefore, in this section we present a
classification model that splits networks into two classes: software-like
networks and non software-like networks.

We chose a simple classification model based on the similarity metric. We need
to define a similarity threshold, so networks with similarity below this
threshold are classified as non software-like, and networks with similarity
greater or equal are classified as software-like. 

Classification models are subject to two types of errors: false negatives
(software networks that are classified as non software-like) and false positives
(non-software networks that are classified as software-like). 

The choice of a threshold is arbitrary.
We want to maximize precision.

\section{Evaluation of Network Models}

In the previous section it was shown that many networks, although scale-free,
can be distinguished from software networks by a triad-based metric. In this
section we show empirically that the three network models previously presented
synthesize networks that are indistinguishable from software networks.

% TODO: use simple past tense

% Is there such a thing as softwareness? Can we reproduce it?

Our research hypothesis is that at least one of the presented models can
synthesize networks that resemble software networks. In order to investigate the
hypothesis we conducted an experiment. First we generated many synthetic
networks, using all three models and different combinations of parameters.
After that we selected the networks that were more similar to software networks.
Finally, we sought patterns in the parameters of the selected networks.

A central issue is what 

We assume that software networks possess a intrinsic quality, that we will call
``softwareness'', that distinguish them from networks in other domains. A
measure of the softwareness is essential to the experiment. One possible measure
is described and evaluated in the following sections. The measured softwareness
of a network represents how likely it is a software network.

% TODO: move to the section on the oracle
We already know that they
produce networks that, just like software networks, are scale-free. This single
property, though, isn't enough to prove the hypothesis, since there are many
known scale-free networks extracted from many distinct domains of knowledge.

To support the analysis of softwareness

% v.  differentiate, differ; distinguish; contrast; discern 

The approach we chose to test our hypothesis was to build a network classifier:
an algorithm that analyses a network and classifies it as a software network or
as a non-software network. If a classifier is capable of classifying with high
accuracy both software and non-software networks, then we can apply it to
synthetic networks and see if they are discernable from software networks.
it gives a fair approximation apply it on synthetic networks. We expect that
some synthetic networks will be so similar to software networks that they will
be classified as software networks.

The experiment we conducted to evaluate the models, described in details in the
following sections, can be summarized as such:
* First of all, we collected both software and non-software networks
* Then, we devised a classifier
* After that, we applied the classifier to our data set and evaluated its
accuracy, precision and recall.
* After we came to an acceptable classifier, we synthesized many networks by
varying the parameters on the models we presented.
* Finally, we classified all synthesized networks as software or non-software.

After that we analysed the results and tried to understand which parameters
of each model contribute mostly to the softwareness of their networks

%First of all, we need a data set containg both software and non-software
%networks. 
%Therefore, in order to prove our hypothesis we must show that, according to some
%criterion, some of at least some of
%the synthetic networks are more similar to software networks than other networks
%are.
%What we are looking for, after all, is an oracle that accepts software networks
%and rejects non-software networks. If the oracle has these two properties, we
%can be confident that it'll accept only synthetic networks that resemble
%software networks.

\subsection{Synthetic Data Set}

We want to investigate if, with a proper choice of parameters, a model is
capable of synthesizing a network that resembles software networks. In this
section we describe which combinations of parameters values were used for each
model.

%In assessing the realism of a model, we want to know if, with a proper choice of
%parameters, the model is capable of producing a realistic network. In order to
%do so, we must generate many networks using different combinations of
%parameters. 

In order to keep the networks in a tractable number, we have set the number of
vertices in each network to 1000, and varied the remaining parameters in
discrete steps such as each parameter assumed no more than 6 values. We have
generated one network for each combination of parameter values.

%Since most parameters are continuous, we had to 
%we chose to threat the number of vertices
%as a constant and vary the other parameters in discrete steps. In all models the
%number of vertices was fixed to 1000. We generated one network for each set of
%parameters.
%Here we describe the criteria we use to choose the parameters for each model.

\subsubsection{BCR+}

We chose five different module dependency networks, which where extracted from
actual dependencies between archives of five different software systems of our
sample. The module dependency networks are shown on Figure
\ref{fig:architectures}. 

\begin{figure*}[!t]
\centering
\includegraphics[width=1.0\textwidth]{architectures}
\caption{Module dependency graphs}
\label{fig:architectures}
\end{figure*}

%02-gef
%04-ibatis
%08-megamek
%16-findbugs
%32-zk

The probabilities p, q and r were given all possible values from 0.0 up to 1.0,
in 0.2 steps, such as the sum of the probabilities was 1. Since the only events
that create vertices are those associated with probabilities p and q, we imposed
the additional restriction that p + q > 0.

For deltain and deltaout we assigned the integer numbers from 0 to 4. TODO: why

Finally, we chose mu from 0.0 to 0.6 in 0.2 steps. It does not make sense to
choose higher values since they mean that there will be more edges connecting
different models.

Total: 9,500 networks.

\subsubsection{LF}

Like BCR, we choose mu ranging from 0.0 to 0.6 in 0.2 steps. For the remaining
parameters we selected values from our sample of software systems. For degexp,
..., we picked the minimum, the median and the max values.

Total: 1,296 networks

\subsubsection{CGW}

p1,p2,p3,p4 ranging from 0.0 to 1.0 in 0.2 steps, with p1 > 0, p1 + p2 + p3 + p4 = 1.0.
e1,e2,e3,e4 in 1, 2, 4, 8 (except that when pi = 0, ei is ignored).
2*p4*e4 >= p1*e1 + p2*e2, so the number of edges created is at least twice
the number of edges removed (to avoid long run times)
alpha in -1, 0, 1, 10, 100, 1000. 
m in 2, 4, 8, 16, 32 (just like bcr and lfr)

Total: 38,790 networks

\subsection{Classifier} % Software network classifier


In order two measure the softwareness of a network, we measure the correlation
between the network's profile and the profile of each of the 65 software
networks in our sample. We then compute the average of these correlations and
call it the S-score, that varies between -1 and 1. The greater the S-score, more
likely that the network is a software network.

In order to build our classifier, we need a threshold for the S-score, so
networks are classified as software networks only when they have S-score greater
or equal to this threshold. We computed the S-score for each of the 65 software
systems and found an average S-score of 0.97 with standard deviation of 0.03.
Using the three-sigma rule from statistics, we chose the threshold as 0.97 - 3 *
0.3, that is 0.88. 

The small standard deviation shows that triad concentration profiles indeed
characterize well the software domain.

TODO: show histogram of S-scores for sw networks (x axis from -1.0 to 1.0)
TODO: Is there need for cross validation in order to choose a threshold?
 - do cross-validation and choose threshold with max precision

\subsubsection{Classifier Validation}

In order to assess if the classifier is capable of distinguishing between
software and non-software networks, we applied it to all the networks from our
sample. 

Most software networks were classified correctly, which is not surprising since
the classifier was based on them.

Precision: X, recall: Y

Curiously, polblog had a very high S-score. This issue requires further 
investigation. Anyway, we cannot deny the possibility that it is really very
software-like.

We could have chosen another value for the threshold, and it would affect the
precision and the recall. For instance, had we chosen 0.82, we would end with
100\% recall and x\% precision. For us, though, it is more important to have a
high precision, so we keep the threshold 0.88.

We also generated networks with the ER model, which is known to generate
networks that are not scale-free and, thus, are very different from software
networks. All the ER networks were classified as non-software.

%Following Milo et al., We then used Pearson's correlation coefficient as a
%similarity measure between two networks. For each software network, we then
%computed the average correlation coefficient (ACC) to the other software
%networks. We've observed that among software networks the ACC is X +- Y.

% avg: 0.97. stdev: 0.03
%We then computed, for each non-software network, the ACC to the 65 software
%networks. By the 3-sigma rule, we use X as the threshold for realistic software
%networks: networks whose ACC is below this threshold are rejected.
%The oracle has X precision and X recall...


\section{Softwareness Evaluation}

We used our classifier ...

Table: Model | Number of Networks | Realistic Networks | Percent \%

Show some graphs: histogram of average correlations for each model.
\ref{fig:histograms}


\begin{figure*}[!t]
\center

\subfigure[Software network]{\includegraphics[width=1.0in]{hist-cgw}}

\subfigure[Non-software network]{\includegraphics[width=1.0in]{hist-bcr}}

\subfigure[Non-software network]{\includegraphics[width=1.0in]{hist-lf}}

\caption{Triad profiles}
\label{fig:histograms}
\end{figure*}

All models produce networks that resemble software networks.  For some
parameters, though, the networks are not realistic.

\subsection{Patterns in Parameters}

1R

Naive Bayes

\subsection{Homogeinity}

Pick realistic networks from a model. Are they similar to each other? (see
standard deviation) In other words: do the parameters make a difference?

Are they similar to networks generated by the other models? In other words: are
the models equivalent?

\section{Threats to validity}

% External validity (EV): can it be generalized?
Structural information isn't enough for a expert do produce a
decomposition (s/he may use data such as names and external documentation). 

Even when considering only structural information, is it true
that experts would find a decomposition similar to the reference decomposition
imposed by the model?

We generated only one network for each set of parameters. (as we've shown, some
parameters are redundant as they do not change significantly the realism)

Some clustering algorithms use weights and they weren't studied here.

EV: We've only studied 65 systems, which is not that much.

We only studied object-oriented systems implemented in Java. Maybe the results
would be different if we studied systems implement in other languagens or using
other paradigms. The choice of a particular technique for extracting
dependencies (static analysis) may also have impact on the structure of the
networks.

% Koschke says that experts decompositions vary by no more than 80? percent.

\section{Conclusion and Future Work}
% revised 2009-09-03

We have shown empirically that network models found in the literature can
synthesize networks that resemble the network of dependencies between classes in
object-oriented systems. This result supports the use of synthetic networks in
the evaluation of software clustering algorithms.
%that operate on class dependency networks. 

The use of synthetic data is common in distributed computing research, but still
underexplored in software engineering research. Because many reverse engineering
tasks rely on dependency data, we expect this work to have impact beyond the
software clustering community.

%Although the use of models and synthetic data is somewhat common in distributed
%computing research, it is underexplored in software engineering research. We
%expect this work to contribute to explore t
%common in distributed computing research, simulation is underexplored
%in software engineering. This work opens a door to the usage of simulation in
%the field of reverse engineering of software in order to evaluate RE 
%algorithms.

We accept that it is important to evaluate the algorithms with real software
networks, but we argue that the use of synthetic networks in a complementary
manner can give researchers new insights about the algorithms. First, the use of
models allows the creation of large test sets, thus diminishing the small sample
effects. Moreover, the networks are created in a controlled way, according to
model parameters, so it is possible to study the behavior of the algorithms with
different parameter values.

In a future work, we intend to use synthetic networks in the evaluation of
software clustering algorithms that were previously studied with real networks
\cite{Wu2005}. After that we will be able to compare the results obtained by
both approaches.

\section{Appendix A: list of networks}

TODO: Show as Table: Name, Vertices, Edges

Software networks:

From SourceForge:
AbaGuiBuilder-1.8
alfresco-labs-deployment-3Stable
aoi272
stendhal-0.74
battlefieldjava-0.1
checkstyle-5.0
dom4j-1.6.1
findbugs-1.3.8
freetts-1.2.2-bin
ganttproject-2.0.9
geoserver-2.0-beta1-bin
geotools-2.5.5-bin
gfp\_0.8.1
hibernate-distribution-3.3.1.GA-dist
hsqldb\_1\_8\_0\_10
iBATIS\_DBL-2.1.5.582
iReport-nb-3.5.1
JabRef-2.5b2-src
jailer\_2.9.9
jalopy-1.5rc3
jasperreports-3.5.2-project
jfreechart-1.0.13
pentaho-reporting-engine-classic-0.8.9.11
jGnash-2.2.0
jgraphpad-5.10.0.2
jmsn-0.9.9b2
juel-2.1.2
JXv3.2rc2deploy
makagiga-3.4
MegaMek-v0.34.3
iFreeBudget-2.0.9
mondrian-3.1.1.12687
oddjob-0.26.0
openxava-3.1.2
pdfsam-1.1.3-out
pjirc\_2\_2\_1\_bin
pmd-bin-4.2.5
proguard4.3
smc\_6\_0\_0
squirrel-sql-3.0.1-base
squirrel-sql-3.0.1-standard
tvbrowser-2.7.3-bin
villonanny-2.3.0.b02.bin
rapidminer-4.4-community
zk-bin-3.6.1

From other places:

ArgoUML-0.28
GEF-0.13-bin
Hl7Comm.1.0.1
IRPF2009v1.1
broker-4.1.5
dbwrench
ec2-api-tools
ermodeller-1.9.2-binary
flyingsaucer-R8
gdata-src.java-1.31.1
guice-2.0
gwt-windows-1.6.4
jai-1\_1\_4-pre-dr-b03-lib-linux-i586-08\_Jun\_2009
jakarta-tomcat-5.0.28-embed
juxy-0.8
myjgui\_0.6.6
peer-4.1.5
subethasmtp-3.1
thinkui\_sqlclient-1.1.2
worker-4.1.5

Other networks:

3 circuit networks (circuit-s208 circuit-s420 circuit-s838)

5 facebook networks (facebook-Caltech36 facebook-Georgetown facebook-Oklahoma
facebook-Princeton facebook-UNC28)

5 language networks (lang-english  lang-french lang-japanese lang-spanish)

43 metabolic networks

3 protein networks (protein-a4j protein-AOR protein-eaw)

2 social networks (social-leader social-prison)

other networks:

polblogs

yeast

beta3sreduced
celegansneural

czech
ecoli-metabolic
