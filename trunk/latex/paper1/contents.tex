\section{Abstract}

Software modularization recovery algorithms automatically recognize a system's
modular structure by analyzing its implementation.
Due to the lack of well document software systems, though, the issue of testing
these algorithms is still underexplored, limiting both their adoption in the
industry and the development of better algorithms.
We propose to rely on software models to produce arbitrarily large test sets. In
this paper we consider three such models and analyze how similar the artifacts
they produce are from artifacts from real software systems.

\section{Introduction}

Development of large-scale software systems is a challenge.

A key to success is the ability to decompose a system into weakly-coupled
modules, so each module can be developed by a distinct team. Failing to do so
results in duplicated code, non-parallelism, one's work impacting another's work
etc.

The ability do modularize depends decisively on a vast knowledge about the
system, how its different parts interact to accomplish the system's goal.

Unfortunately, in the case of legacy systems, such knowledge isn't available.
Depending on its size, it might take months to understand the system so well as
to find a good modularization.

POR ISSO SURGIRAM software modularization recovery algorithms, also known as
software clustering algorithms or software architecture recovery algorithms. In
its most common flavor, these algorithms analyze the dependencies between
implementation components, such as classes, and then group them into modules
such as there are few dependencies between classes in distinct modules.

Software modularization recovery algorithms can, therefore, do in minutes what a
person would spend weeks or months. The question is: are the found
modularizations good? Are they similar to what a person would find? To answer
this question it's essencial to perform empirical evaluations envolving systems
with known reference modularizations.

The empirical evaluations consist of selecting a collection of systems with
known reference modularizations and then applying the algorithms to the systems.
The modularizations found by the algorithms are then compared to the reference
decompositions by a metric such as MoJo CITE or PrecisionRecall CITE.

Unfortunately there are few systems with known reference modularizations and,
because to obtain reference modularizations is costly, there are few empirical
studies, and most of them consider a couple of small and medium systems.

We therefore propose to use synthetic, i.e., computer-generated, software
dependency networks, to evaluate software modularization recovery algorithms.
These networks are generated by parametrizable models and have an embedded
reference modularizations. The goal of an algorithm is, thus, to find
modularizations that are similar to the reference modularization embedded in the
network. With this approach we can CONTAR COM a large volume of test data that
is composed of networks of different sizes and controllable characteristics.

Of course the success of this approach depends on the realism of the synthetic
networks, ie, how well they resemble networks extracted from real software
systems. In this paper we study three models and show that all of them are, by
means of a careful parameter choosing, capable of producing realistic software
networks.

The remaining sections are organized as .... Section 2, ...

%alike, resembling, exchangeable, indistiguible

\section{Software Networks and Software Clustering}

directed graph, (un)weighted

\section{Complex Networks}

Complex network theory found many scale-free networks

Software dependency networks are scale free. CITE Valverde, Myers

Scale free means ... $N(k) \sim k^{-\gamma}$

\section{Models}

Many scale-free models have been proposed. Only a few, though, produce modular
networks. 

Glossary: preferential attachment

\subsection{ER (Erdos-Renyi)}

\subsection{LF}

Directed weighted networks with overlapping modules.

\subsection{BCR plus}

We propose an extension to BCR model... which was proposed to model the links
between web pages.

The BCR+ model builds networks that respect a module-dependency network, that
is, a network that contains modules and allowed dependencies between modules. 
In the component dependency network, an edge between two vertices is only
allowed if there exist an edge between their respective modules.

First of all, the model generates a network that is a copy of the module
dependency network, each vertex belonging to its respective module. Then the
algorithm runs iteratively, and on each iteration it executes an operations that
modifies the network. It can add a new vertex together with an edge connecting it
to an old vertex, or it can add an edge between two existing vertices. The
choice of vertices, though, is not fully random. The probability that a
particular vertex v is choosen, P(v), is proportional to a function that involves the
in-degree or the out-degree of the node. We say that a vertex is chosen within a
set V according to a function f if

P(choose v) = f(v) / sum(v in V, f(v))

The denominator is a normalizing factor that assures that the probabilities sum
to 1.

picks a event that evolves
the network. Each event has an associated probability.

* With probability p, a new vertex is added to the network, together with an
edge from the new vertex to an existing vertex, chosen preferentially according
to .... Considering din = 0, this means that a vertex with indegree = 8 is
twice more likely to receive the edge than a vertex with indegree = 4. The
parameter din can be used to alleviate the handicap. (If din = 4, the first
vertex will only be 3/2 more likely to receive the edge). The new vertex is
put on the same cluster as the old vertex.
* With probability q, a new vertex is added to the network, together with an
edge from an existing vertex, chosen preferentially accordingly to dout +
outdegree, to the new vertex. The new vertex is put on the same cluster as the
old vertex. This case is similar to the previous case.
* With probability r, a new edge is added between two vertices, v and w. v is
chosen according to .... After that, with prob X, w is chosen from one of the
modules connected to v's module, else it's chosen from v's own module. In any
case, the exact choice is made according to ...

It's a growth model, that is, it can take as input an existing network and
evolve it.

TODO: Argument: why the imposed modules are natural modules?

\subsection{CGW}

The CGW was proposed to model the evolution of software systems organized in
modules. It was proven formally to generate scale-free networks.

The CGW model is similar to BCR+ in which it is a growth model. The initial
network is composed of two vertices with a directed edge between, belonging
to the same module. The other M - 1 modules are initially empty.

Then, at each iteration, the algorithm executes one of the following events:

* With probability p1, one vertex is added to a randomly-chosen module, together
with e1 edges from p1 to vertices chosen according to module-based preferential
attachment.

* With probability 

Accounts for the removal of edges. Growth model.

\section{Experimental Setup}

We want to investigate if the models can produce networks that resemble software
networks. We know that they share with software networks the scale-free
property. This is not enough, since many real networks share this property. So
we looked for a method to differentiate between software networks and other
networks.

What we are looking for, after all, is an oracle that accepts software networks
and rejects non-software networks. If the oracle has these two properties, we
can be confident that it'll accept only synthetic networks that resemble
software networks.

In a recent work, Milo et al. proposed the study of triads in order to
characterize different classes of networks. We thus follow their work here.

Triads are...

Figure 1a show triads for a software system... Figure 2a show triads for network
from domain X.

\subsection{Software Network Extraction}

We've collected 65 systems written in Java and X networks from many domains,
such as sociology, biology, technology and linguistics.
Table 1 ...

\subsection{Oracle Training and Evaluation}

We then used Pearson's correlation coefficient as a similarity measure between
two networks. For each software network, We then computed the average
correlation coefficient (ACC) to the other software networks. We've observed
that among software networks the ACC is X +- Y.

We then computed, for each non-software network, the ACC to the 65 software
networks. By the 3-sigma rule, we use X as the threshold for realistic software
networks: networks whose ACC is below this threshold are rejected.

The oracle has X precision and X recall...

\subsection{Network Synthesis}

In assessing the realism of a model, we want to know if, with a proper choice of
parameters, the model is capable of producing a realistic network. In order to
do so, we must generate many networks using different combinations of
parameters. 

Since most of the parameters can assume infinite distinct values, we chose to
fix some of them and vary the others in discrete steps. In all models the number
of vertices was fixed to 1000. We generated one network for each set of
parameters. Here we describe the criteria we use to choose the parameters for
each model.

\subsubsection{BCR+}

We chose five different module dependency networks, which where extracted from
actual dependencies between modules of five different software systems of our
sample, ranging from 2 to 32 modules. The module dependency networks are shown
on Figure XXX. 
%02-gef
%04-ibatis
%08-megamek
%16-findbugs
%32-zk

The probabilities p, q and r were given all possible values from 0.0 upto 1.0,
in 0.2 steps, such as the sum of the probabilities was 1. Since the only events
that create vertices are those associated with probabilities p and q, we imposed
the additional restriction that p + q > 0.

For deltain and deltaout we assigned the integer numbers from 0 to 4. TODO: why

Finally, we chose mu from 0.0 to 0.6 in 0.2 steps. It does not make sense to
choose higher values since they mean that there will be more edges connecting
different models.

Total: 9,500 networks.

\subsubsection{LF}

Like BCR, we choose mu ranging from 0.0 to 0.6 in 0.2 steps. For the remaining
parameters we selected values from our sample of software systems. For degexp,
..., we picked the minimum, the median and the max values.

Total: 1296 networks

\subsubsection{CGW}

p1,p2,p3,p4 ranging from 0.0 to 1.0 in 0.2 steps, with p1 > 0, p1 + p2 + p3 + p4 = 1.0.
e1,e2,e3,e4 in 1, 2, 4, 8 (except that when pi = 0, ei is ignored).
2*p4*e4 >= p1*e1 + p2*e2, so the number of edges created is at least the 
double of the number of edges removed
alpha in -1, 0, 1, 10, 100, 1000. 
m in 2, 4, 8, 16, 32 (just like bcr and lfr)

Total: 38790 networks

\subsubsection{ER}

Number of edges chosen randomly in the range ...

\section{Realism Evaluation}

Table: Model | Number of Networks | Realistic Networks | Percent \%

Show some graphs: histogram of average correlations for each model.

As expected, ER produces non-realistic software networks.

All models produce networks that resemble software networks.  For some
parameters, though, the networks are not realistic.

\subsection{Patterns in Parameters}

1R

Naive Bayes

\subsection{Homogeinity}

Pick realistic networks from a model. Are they similar to each other? (see
standard deviation) Are they similar to networks generated by the other models?

\section{Threads to validity}

% External validity (EV): can it be generalized?
Structural information isn't enough for a expert do produce a
decomposition (s/he may use data such as names and external documentation). 

Even when considering only structural information, is it true
that experts would find a decomposition similar to the reference decomposition
imposed by the model?

We generated only one network for each set of parameters. (as we've shown, some
parameters are redundant as they do not change significantly the realism)

Some clustering algorithms use weights and they weren't studied here.

EV: We've only studied 65 systems, which is not that much.

We only studied object-oriented systems implemented in Java. Maybe the results
would be different if we studied systems implement in other languagens or using
other paradigms. The choice of a particular technique for extracting
dependencies (static analysis) may also have impact on the structure of the
networks.

# Koschke says that experts decompositions vary by no more than 80? percent.

\section{Conclusion and Future Work}

The use of models lead to a more controlled way of experimented, and allows us
to play with a large data set and a wide variety of characteristics. Although
we have shown that the models studied can produce realistic networks according
to one criterium, we can't just discard the experimentation with real software
systems, as they are the subjects in which algorithms will be used in the
real world. In particular, it's an open question whether the models can 
reproduce any software system or if they just generate a subset of systems.
We believe that there are software systems that cannot be simulated by none
of these models.

Although common in distributed computing research, simulation is underexplored
in software engineering. This work opens a door to the usage of simulation in
the field of reverse engineering of software in order to evaluate RE 
algorithms.

Future work: to apply algorithms to the networks and compare the results with
results found in the literature.

\section{Appendix A: list of networks}

Software networks:

Name | Source

Other networks:

Description | Source



